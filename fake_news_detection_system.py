# -*- coding: utf-8 -*-
"""Fake news detection system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SwiUjZ-rNOuhfMODISRbci6CN3osSJUg
"""

# ======================================
# STEP 1: Install Dependencies
# ======================================

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
import cv2
from sklearn.metrics import accuracy_score, classification_report

# ======================================
# STEP 2: Load Kaggle Fake News Dataset
# ======================================
import kagglehub

# Download dataset (already downloaded by you, but keeping here for automation)
path = kagglehub.dataset_download("clmentbisaillon/fake-and-real-news-dataset")

df_fake = pd.read_csv(os.path.join(path, "Fake.csv"))
df_true = pd.read_csv(os.path.join(path, "True.csv"))

df_fake["label"] = 0  # Fake = 0
df_true["label"] = 1  # True = 1

df = pd.concat([df_fake, df_true], axis=0).sample(frac=0.2, random_state=42) # small subset for demo
df = df[["text", "label"]]

print(df.head())
print("Dataset size:", df.shape)

# ======================================
# STEP 3: Text Preprocessing
# ======================================
X = df["text"].values
y = df["label"].values

# Tokenization
max_words = 10000
max_len = 200
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X)

X_seq = tokenizer.texts_to_sequences(X)
X_pad = pad_sequences(X_seq, maxlen=max_len)

X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)

# ======================================
# STEP 4: LSTM Model for Fake News Detection
# ======================================
model_text = Sequential([
    Embedding(max_words, 128, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.3),
    Bidirectional(LSTM(32)),
    Dense(64, activation="relu"),
    Dropout(0.3),
    Dense(1, activation="sigmoid")
])

model_text.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
history = model_text.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)

# Evaluate
y_pred = (model_text.predict(X_test) > 0.5).astype("int32")
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# ======================================
# STEP 5: Image Fake/True Detection (CNN Demo)
# ======================================
# ‚ö†Ô∏è Note: No official "fake vs real news images" dataset exists on Kaggle in this repo.
# For demo, let's assume you create 2 folders: /content/images/fake and /content/images/real

# Create dummy directories and files for demonstration
import os
import numpy as np
from PIL import Image

dummy_base_dir = "/content/images"
fake_dir = os.path.join(dummy_base_dir, "fake")
real_dir = os.path.join(dummy_base_dir, "real")

os.makedirs(fake_dir, exist_ok=True)
os.makedirs(real_dir, eist_ok=True)
x
# Create some dummy images
def create_dummy_image(filepath, size=(128, 128)):
    img = Image.fromarray(np.random.randint(0, 255, size, dtype='uint8'), 'L').convert('RGB')
    img.save(filepath)

for i in range(10):
    create_dummy_image(os.path.join(fake_dir, f"fake_img_{i}.png"))
    create_dummy_image(os.path.join(real_dir, f"real_img_{i}.png"))


from tensorflow.keras.preprocessing.image import ImageDataGenerator

img_height, img_width = 128, 128
batch_size = 32

datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_gen = datagen.flow_from_directory(
    dummy_base_dir, target_size=(img_height, img_width),
    batch_size=batch_size, class_mode="binary", subset="training"
)

val_gen = datagen.flow_from_directory(
    dummy_base_dir, target_size=(img_height, img_width),
    batch_size=batch_size, class_mode="binary", subset="validation"
)

model_img = Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation="relu", input_shape=(img_height, img_width, 3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation="relu"),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    Dense(128, activation="relu"),
    Dropout(0.3),
    Dense(1, activation="sigmoid")
])

model_img.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model_img.fit(train_gen, validation_data=val_gen, epochs=3)

# ======================================
# STEP 6: Save Models
# ======================================
model_text.save("text_model.h5")
model_img.save("image_model.h5")

# Commented out IPython magic to ensure Python compatibility.
# # ======================================
# # STEP 7: Streamlit Deployment
# # ======================================
# %%writefile app.py
import streamlit as st
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import cv2
# 
# # Load models

# 

from google.colab import files

# Download text model
files.download("text_model.h5")

# Download image model
files.download("image_model.h5")

import streamlit as st
from PIL import Image
import numpy as np
import os
import gdown
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

st.set_page_config(page_title="Fake News Detection", layout="centered")
st.title("üåê Fake News Detection (Text + Image)")

# --- Create folder for models ---
os.makedirs("model", exist_ok=True)

# --- Download models if not present ---
text_model_id = "1mq6HKFdE2cjNnrhp5f4ROKA4RRCrFxIr"
image_model_id = "1uVzRVVMEsdIxw7pItqiWN3RI_Mgp8GRQ"

text_model_path = "/content/text_model.h5"
image_model_path = "/content/image_model.h5"

if not os.path.exists(text_model_path):
    gdown.download(f"https://drive.google.com/uc?id={image_model_id}", text_model_path, quiet=False)

if not os.path.exists(image_model_path):
    gdown.download(f"https://drive.google.com/uc?id={image_model_id}", image_model_path, quiet=False)

# --- Load models ---
@st.cache_resource
def load_models():
    text_model = load_model(text_model_path)
    image_model = load_model(image_model_path)
    return text_model, image_model

text_model, image_model = load_models()

# --- Tabs for Text and Image ---
tab1, tab2 = st.tabs(["Text News", "Image News"])

with tab1:
    st.header("üì∞ Text-based News Detection")
    news_text = st.text_area("Paste news article here:")
    if st.button("Predict Text"):
        if news_text.strip() == "":
            st.warning("Please enter some news text!")
        else:
            tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
            tokenizer.fit_on_texts([news_text])
            seq = tokenizer.texts_to_sequences([news_text])
            padded = pad_sequences(seq, maxlen=500, padding='post', truncating='post')

            pred = text_model.predict(padded)[0][0]
            if pred > 0.5:
                st.error("‚ö†Ô∏è This news is likely **Fake**")
            else:
                st.success("‚úÖ This news is likely **True**")

with tab2:
    st.header("üñº Image-based News Detection")
    uploaded_file = st.file_uploader("Upload a news image", type=["png", "jpg", "jpeg"])
    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("RGB")
        st.image(image, caption="Uploaded Image", use_column_width=True)

        # Preprocess image
        img = image.resize((224, 224))
        img_array = np.array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        pred = image_model.predict(img_array)[0][0]
        if pred > 0.5:
            st.error("‚ö†Ô∏è This news image is likely **Fake**")
        else:
            st.success("‚úÖ This news image is likely **True**")

